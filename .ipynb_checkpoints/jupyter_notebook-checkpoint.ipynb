{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. Waterpoints are an invaluable source of water for household needs such as drinking and washing. These are strategically situated to help rural communities and given their importance, need to be constantly available for the communities. There are many waterpoints already established in the country, however, some are in need of repair while others have failed altogether. \n",
    "\n",
    "The primary task of this notebook is to predict the condition of a water well, given detailed information about the pumps. Ultimately, we will create a machine learning model to accurately determine contributing factors that can preemptively determine pump repair and/or failure. Knowing and addressing the most important factors are upkept will ensure pumps continue to provide clean water to people in Tanzania.\n",
    "\n",
    "This information will prove useful to the entity responsible for the upkeep and maintenance of such waterpoints, as well as external donors that may be funding the operational costs associated with the equipment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# training_values.csv = features/dimensions we will be examining\n",
    "# training_labels.csv = condition of the waterpoint \n",
    "\n",
    "training_values = pd.read_csv('training_values.csv')\n",
    "training_labels = pd.read_csv('training_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes on the id so we can match the correct condition to the correct waterpoint\n",
    "df = training_values.set_index('id').join(training_labels.set_index('id'), how='inner')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:** We will need to strategically reduce the current number of features, using domain knowledge and machine learning methods. Detailed information about what each feature means is defined in the `feature_descriptions.txt` data dictionary. We will examine the specific value counts for each column, shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:** `amount_tsh` (amount of water available to waterpoint) appears to have an outlier(s) as the _max_ values is significantly higher than the _mean_. We have no information on the interpretation of `num_private`, but has values from 0 to 1776, which is a signficant range. We also see 0 values for `construction_year`, which means we'll have to take that into consideration when dealing with missing/null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain normalized value counts for each column\n",
    "for col in df.columns:\n",
    "    print(\" \")\n",
    "    print(f'---{col}---')\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:** These results provide a great starting point in which features might be heavily correlated with one another or present similar data, like:\n",
    "- `longitude`/`latitude` and `subvillage` and `region` and `region_code` and `district_code` and `lga/ward`\n",
    "- `scheme_management` and `scheme_name`\n",
    "- `extraction_type` and `extraction_type_group` and `extraction_type_class`\n",
    "- `management` and `management_group`\n",
    "- `water_quality` and `quality_group`\n",
    "- `quantity` and `quantity_group`\n",
    "- `source` and `source_type` and `source_class`\n",
    "- `waterpoint_type` and `waterpoint_type_group`\n",
    "- `payment` and `payment_type`\n",
    "\n",
    "> These datapoints don't have any relevance or impact on the functionality of the wellpoint:\n",
    "- `id`\n",
    "- `wpt_name`\n",
    "- `recorded_by` (same value for all datapoints)\n",
    "\n",
    "> As we do not understand the feature description for `num_private` or `public_meeting` - it will be excluded from further analaysis.\n",
    "\n",
    "> It also provides the three labels with which we will categorize the waterpoints:\n",
    "- `functional`\n",
    "- `non functional`\n",
    "- `functional needs repair`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine dataset for NaN values\n",
    "df.isna().sum()\n",
    "\n",
    "# data.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check original dataframe for duplicate values by unique `id` and `construction_year`\n",
    "df[df.duplicated(['id','construction_year'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a boxplot to look for outliers (remove non-applicable columns)\n",
    "cols = [col for col in df.columns if df[col].dtype != 'O']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18,9))\n",
    "for xcol, ax in zip(cols, axes.flatten()):\n",
    "    boxplot = df.boxplot(column=xcol, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine outliers in further detail\n",
    "display(df[df.amount_tsh >= 190000])\n",
    "display(df[df.population >= 20000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: None of the datapoints look like it was incorrectly entered and make reasonable sense. We'll include these outliers for now and determine at a later time, if removing them aids in model accuracy. Additionally, these waterpoints could be geographically positioned in a place with a large basin. Also populations tend to congregate, so it is not unreasonable to have outliers in available water at certain waterpoints or large populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set font and color scheme\n",
    "plt.style.use('bmh')\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of Pumps and Population Around Pumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show normalized count of different status groups\n",
    "print(\"Functionality of the Wells\")\n",
    "display(df.status_group.value_counts(normalize=True))\n",
    "ax = sns.countplot(df['status_group'])\n",
    "ax.set(ylabel='# of waterpumps')\n",
    "plt.show()\n",
    "\n",
    "# show populations affected by each status_group\n",
    "print(\"Population Around the Wells\")\n",
    "display(df.groupby(df['status_group'])['population'].sum())\n",
    "\n",
    "f,ax1 = plt.subplots(figsize =(6,4))\n",
    "pop_pump = df.groupby(df['status_group'])['population'].sum()\n",
    "pop_pump.plot(x='status_group', y = 'population', kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('status_group')\n",
    "plt.ylabel('population (millions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: Within our available data, over half **(54.3%)** of the waterpoints are functional, with **38.4%** being non-functional. <br><br> Additionally, there are **3,880,455** people that do not have access to a functional waterpoint. This is indication of a severe shortage of functionable water pumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Amount of Water Per Status Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# group the data by the average amount of water per status_group\n",
    "avg_water = df[['amount_tsh','status_group']].groupby(['status_group']).mean()\n",
    "\n",
    "# average amount of water per non functional pump\n",
    "nonfunc_avg_amt = df[df['status_group'] == 'non functional']['amount_tsh'].mean()\n",
    "print('Average amount of water per non-functional pump: %.2f' % nonfunc_avg_amt)\n",
    "\n",
    "# plot the grouping\n",
    "f,ax1 = plt.subplots(figsize =(6,4))\n",
    "avg_water.plot.bar(ax=ax1)\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Amount_tsh')\n",
    "plt.title('Average Amount of Water')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average amount of water per pump when the quantity is dry\n",
    "dry_avg_amt = df[df['quantity'] == 'dry']['amount_tsh'].mean()\n",
    "print('Average amount of water per pump when quantity is dry: %.2f' % dry_avg_amt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: The data intuitively makes sense - the non-functional pumps have less availabe water on average per pump and hence, may be why they are non-functional. In other words, some of these non-functional pumps may be dry and run out of available water."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of Pumps by Funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the status_group of pumps grouped by the funder\n",
    "pd.crosstab(df['funder'],df['status_group']).sort_values('non functional',ascending=False).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: In the table above, the darker green colors represent the higher values in that column. The `Government of Tanzania` is by far the greatest funder by volume of pumps, but also has a significant number of their pumps being non-functional. `World Vision` has an interesting ratio of **753:372** of **functional:non-functional** pumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of Pumps by Funder (Percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine percentages of functional vs non-functional pumps by funder\n",
    "list_of_funders = pd.DataFrame(pd.crosstab(df['funder'], df['status_group']))\n",
    "\n",
    "# set minimum # of pumps funded to filter results for only experienced funders\n",
    "threshold_number_of_pumps = 500\n",
    "considerable_funders = list_of_funders[list_of_funders['functional'] + list_of_funders['functional needs repair'] + list_of_funders['non functional'] >= threshold_number_of_pumps]\n",
    "\n",
    "# obtain percentages for each funder\n",
    "percentages_funders = considerable_funders.apply(lambda r: r/r.sum()*100, axis=1)\n",
    "\n",
    "# obtain ratio of functional vs non-functional pumps\n",
    "percentages_funders['delta'] = percentages_funders['functional'] - percentages_funders['non functional']\n",
    "percentages_funders.sort_values('delta', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: The `Government of Tanzania` is the greatest funder, but we see a pretty poor delta of **-10.4** when it comes to functionality. Pumps funded by`Germany Republi` and `Private Individual` have significantly higher ratios of functionality than other funders who have funded over 500 pumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of Pumps by Installer (Percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the percent stacked bar graph of top 10 installers broken down by the status_group\n",
    "ax = pd.crosstab(df['installer'], df['status_group']).sort_values('non functional',ascending=False).apply(lambda r: r/r.sum()*100, axis=1)[:10]\n",
    "ax_1 = ax.plot.bar(figsize=(8,8),stacked=True, rot=0)\n",
    "display(ax)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.1, 1.0), title=\"status\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Installer')\n",
    "plt.ylabel('Percent Distribution')\n",
    "\n",
    "for rec in ax_1.patches:\n",
    "    height = rec.get_height()\n",
    "    ax_1.text(rec.get_x() + rec.get_width() / 2, \n",
    "              rec.get_y() + height / 2,\n",
    "              \"{:.0f}%\".format(height),\n",
    "              ha='center', \n",
    "              va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: The top 10 installers (by # of water pumps installed) have a high non-functional percentage ranging from **29%** (Commu) to **72%** (Central Government)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of Pumps by Extraction Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the status_group and # of pumps grouped by the extraction_type_group\n",
    "ct = pd.crosstab(df['extraction_type_group'],df['status_group'])\n",
    "extraction_percentages = ct.apply(lambda r: r/r.sum()*100, axis=1).sort_values('functional',ascending=False)\n",
    "display(extraction_percentages)\n",
    "\n",
    "ct.sort_values('functional',ascending=False).plot.barh(figsize=(8,8), stacked=True)\n",
    "plt.legend(title='status')\n",
    "plt.ylabel('# of pumps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**: The majority of the pumps extract water via gravity methods. `afridev` and `nira/tanira` extraction methods have the highest functionality rate **67.8%** and **66.5%**, respectively), while `other` methods have the lowest functionality rate at **16.0%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create function to reduce values in a column\n",
    "def feature_reduction(df, column):\n",
    "    \n",
    "    x = 0\n",
    "    selection = 0\n",
    "    # determine the values that encapsulate the top \"threshold\" of the data\n",
    "    threshold = 0.97\n",
    "    \n",
    "    while selection < threshold:\n",
    "        x += 1\n",
    "        selection = df[column].value_counts(normalize=True)[:x].values.sum()   \n",
    "    \n",
    "    # replace the values of the 5% with placeholder of \"other\"\n",
    "    selected_values = df[column].value_counts(normalize=True)[:x].index\n",
    "    df.loc[~df[column].isin(selected_values), column] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(df, selected_features, test_data=False):\n",
    "    \n",
    "    # reduce the # of unique values in certain features using the function, \"feature_reduction\"\n",
    "    feature_list = ['installer', 'funder', 'lga', 'ward', 'subvillage']\n",
    "\n",
    "    for col in feature_list:\n",
    "        feature_reduction(df,col)\n",
    "    \n",
    "    # use selected features for analysis\n",
    "    data = df[selected_features]\n",
    "    \n",
    "    # fill in relevant NaN values with placeholder value of 'unknown'\n",
    "    # df.funder = df.funder.fillna(\"unknown\")\n",
    "    # df.installer = df.installer.fillna(\"unknown\")\n",
    "    # df.scheme_management = df.scheme_management.fillna(\"unknown\")\n",
    "    # df.permit = df.permit.fillna(\"unknown\")\n",
    "    # df.subvillage = df.subvillage.fillna(\"unknown\")\n",
    "    # df.public_meeting = df.public_meeting.fillna(\"unknown\")\n",
    "    # df.scheme_name = df.scheme_name.fillna(\"unknown\")\n",
    "    \n",
    "    # for unknown construction years, set the values as actual NaN values\n",
    "    # and set the median for the missing data\n",
    "    # data.construction_year = data.construction_year.replace(0, np.nan).fillna(data.construction_year.median())\n",
    "\n",
    "    # remove NaN rows for which construction year is unknown\n",
    "    # data.dropna(subset= ['construction_year'], inplace=True)\n",
    "\n",
    "    # convert every data value to a string, if the feature column is an object datatype\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'O':\n",
    "            data.loc[:, col] = data[col].apply(str)\n",
    "    \n",
    "    if test_data == False:\n",
    "        features = data.drop('status_group', axis=1)\n",
    "        labels = data.status_group\n",
    "\n",
    "        return features, labels\n",
    "    \n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(train_set, test_set):    \n",
    "    # OneHotEncode categorical variables and create dataframe of features\n",
    "    cat_features = [col for col in train_set.columns if train_set[col].dtype in [np.object]]\n",
    "    X_train_cat = train_set.loc[:, cat_features]\n",
    "    X_test_cat = test_set.loc[:, cat_features]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "    \n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    ohe_X_train = pd.DataFrame(X_train_ohe.todense(), columns=columns, index=train_set.index)\n",
    "    ohe_X_test = pd.DataFrame(X_test_ohe.todense(), columns=columns, index=test_set.index)\n",
    "    \n",
    "    return ohe_X_train, ohe_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(df, selected_features):\n",
    "    # preprocess the training data\n",
    "    features, labels = data_preparation(df, selected_features)\n",
    "    \n",
    "    # split into training and validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=73)\n",
    "    \n",
    "    # OneHotEncode training and validation sets\n",
    "    ohe_X_train, ohe_X_test = one_hot_encode(X_train, X_test)\n",
    "    \n",
    "    return ohe_X_train, y_train, ohe_X_test, y_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice relevant features for data analysis\n",
    "selected_features = ['amount_tsh', 'date_recorded', 'longitude', 'latitude', 'funder', 'gps_height', 'installer', \n",
    "                     'basin', 'region', 'population', 'scheme_management', 'permit', 'construction_year', \n",
    "                     'extraction_type_class', 'management', 'payment_type', 'quality_group', 'quantity_group', \n",
    "                     'waterpoint_type_group', 'lga', 'ward', 'extraction_type', 'subvillage', 'source', \n",
    "                     'source_type', 'source_class', 'waterpoint_type', 'quantity', 'water_quality', 'payment', \n",
    "                     'management_group','extraction_type_group', 'public_meeting', 'district_code', 'region_code',\n",
    "                     'status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute preprocessing_data function\n",
    "ohe_X_train, y_train, ohe_X_test, y_test, features = preprocessing_data(df, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_log(x_train, y_train, x_test, y_test):    \n",
    "    logreg = LogisticRegression(fit_intercept=False, solver='liblinear', n_jobs=-1)\n",
    "    model_log = logreg.fit(x_train, y_train)\n",
    "\n",
    "    # print the accuracy on test set\n",
    "    print(f'pipeline test accuracy:{model_log.score(x_test, y_test) :.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_log(ohe_X_train, y_train, ohe_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRY function to train different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train models\n",
    "def model_train(X_train, y_train, model, grid, X_test=None, y_test=None, test_model=False):\n",
    "    # construct a pipeline\n",
    "    pipe = Pipeline([('ss', StandardScaler()),\n",
    "                    ('model', model)])\n",
    "\n",
    "    # Define a grid search\n",
    "    grid_model = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=3,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "    # Fit the pipelines\n",
    "    grid_model.fit(X_train, y_train)\n",
    "\n",
    "    if test_model == False:\n",
    "        best_parameters = grid_model.best_params_\n",
    "        \n",
    "        print(\"Grid Search found the following optimal parameters: \")\n",
    "        for param_name in sorted(best_parameters.keys()):\n",
    "            print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "        print(f'pipeline test accuracy:{grid_model.score(X_test, y_test) :.2%}')\n",
    "    \n",
    "    return grid_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier(random_state=73)\n",
    "# grid = [{'model__n_estimators': [10, 30, 100, 200],\n",
    "#          'model__criterion': ['gini', 'entropy'],\n",
    "#          'model__max_depth': [None, 2, 6, 10],\n",
    "#          'model__min_samples_split': [2, 5, 10],\n",
    "#          'model__min_samples_leaf': [1, 3, 6]}]\n",
    "rfc_grid = [{'model__n_estimators': [200],\n",
    "         'model__criterion': ['entropy'],\n",
    "         'model__max_depth': [None],\n",
    "         'model__min_samples_split': [10],\n",
    "         'model__min_samples_leaf': [1]}]\n",
    "model_rfc = model_train(ohe_X_train, y_train, rfc_model, rfc_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model_rfc.best_estimator_.named_steps[\"model\"].feature_importances_, index=ohe_X_train.columns)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize =(12,4))\n",
    "feat_importances.nlargest(10).plot(kind='barh', ax=ax1)\n",
    "feat_importances.nsmallest(10).plot(kind='barh', ax=ax2)\n",
    "ax1.title.set_text('Most Important Features')\n",
    "ax2.title.set_text('Least Important Features')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=73)\n",
    "log_grid = [{'model__C': [np.logspace(-4, 4, 1)], \n",
    "         'model__penalty': ['l1', 'l2']}]\n",
    "\n",
    "model_log = model_train(ohe_X_train, y_train, log_model, log_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Principal Component Analysis (dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# determine the number of principal components to explain 95% of the variance\n",
    "pca = PCA()\n",
    "pca.fit_transform(ohe_X_train)\n",
    "\n",
    "# determine the number of features to capture 95% of the variance\n",
    "total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "n_over_95 = len(total_explained_variance[total_explained_variance >= .95])\n",
    "n_to_reach_95 = ohe_X_train.shape[1] - n_over_95 + 1\n",
    "print(\"Number features: {}\\tTotal Variance Explained: {}\".format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))\n",
    "\n",
    "# subset the dataset to these principal components which capture 95% of the overall variance\n",
    "# reproject the dataset into a lower-dimensional space using PCA\n",
    "pca = PCA(n_components=n_to_reach_95)\n",
    "X_pca_train = pca.fit_transform(ohe_X_train)\n",
    "X_pca_test = pca.transform(ohe_X_test)\n",
    "\n",
    "pca.explained_variance_ratio_.cumsum()[-1]\n",
    "# #### refit a model on the compressed dataset ####\n",
    "# clf = svm.SVC(gamma='auto')\n",
    "# train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "# test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "# print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to train a model with PCA\n",
    "def model_train_pca(model, grid):\n",
    "    # construct a pipeline\n",
    "    pipe = Pipeline([('pca', PCA(n_components=403, random_state=73)),\n",
    "                    ('model', model)])\n",
    "\n",
    "    # Define a grid search\n",
    "    gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=3)\n",
    "\n",
    "    # Fit the pipelines\n",
    "    gridsearch.fit(X_pca_train, y_train)\n",
    "\n",
    "    best_parameters = gridsearch.best_params_\n",
    "\n",
    "    print(\"Grid Search found the following optimal parameters: \")\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # Print the accuracy on test set\n",
    "    print(f'pipeline test accuracy:{gridsearch.score(X_pca_test, y_test) :.2%}')\n",
    "    \n",
    "    return gridsearch\n",
    "\n",
    "model = LogisticRegression(random_state=73)\n",
    "grid = [{'model__C': np.logspace(-4, 4, 50), \n",
    "         'model__penalty': ['l1', 'l2']}]\n",
    "\n",
    "model_log = model_train_pca(model, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbors with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "# grid = [{'model__n_neighbors': [11, 19],\n",
    "#          'model__weights': ['uniform', 'distance'],\n",
    "#          'model__metric': ['euclidean', 'manhattan']}]\n",
    "knn_grid = [{'model__n_neighbors': [5,11],\n",
    "         'model__weights': ['uniform'],\n",
    "         'model__metric': ['minkowski']}]\n",
    "\n",
    "model_knn = model_train(ohe_X_train, y_train, knn_model, knn_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(random_state=73)\n",
    "# grid = [{'model__C' : np.linspace(.1, 10, num=2),\n",
    "#         'model__gamma' : np.linspace(10**-3, 5, num=2),\n",
    "#         'model__kernel': ['linear','rbf', 'poly', 'sigmoid']}]\n",
    "svm_grid = [{'model__C' : [1],\n",
    "        'model__gamma' : ['scale'],\n",
    "        'model__kernel': ['rbf']}]\n",
    "\n",
    "model_svm = model_train(ohe_X_train, y_train, svm_model, svm_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree_model = DecisionTreeClassifier(random_state=73)\n",
    "decisiontree_grid = [{'model__criterion': ['gini', 'entropy'],\n",
    "         'model__max_depth': [None, 2, 3, 4, 5, 6],\n",
    "         'model__min_samples_split': [2, 5, 10],\n",
    "         'model__min_samples_leaf': [1, 2, 3, 4, 5, 6]}]\n",
    "\n",
    "model_decisiontree = model_train(ohe_X_train, y_train, decisiontree_model, decisiontree_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model = AdaBoostClassifier(random_state=73)\n",
    "adaboost_grid = [{'model__n_estimators': [30, 50, 70],\n",
    "         'model__learning_rate': [1.0, 0.5, 0.1]}]\n",
    "\n",
    "model_adaboost = model_train(ohe_X_train, y_train, adaboost_model, adaboost_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgb.XGBClassifier(random_state=73)\n",
    "xgboost_grid = [{'model__learning_rate': [0.05, 0.1],\n",
    "         'model__max_depth': [3, 6],\n",
    "         'model__min_child_weight': [5, 10],\n",
    "         'model__subsample': [0.3, 0.7],\n",
    "         'model__n_estimators': [5, 30, 100, 250]}]\n",
    "\n",
    "model_xgboost = model_train(ohe_X_train, y_train, xgboost_model, xgboost_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradientboosting_model = GradientBoostingClassifier(random_state=73)\n",
    "gradientboosting_grid = [{'model__loss':[\"deviance\"],\n",
    "         'model__learning_rate': [0.01, 0.2],\n",
    "         'model__min_samples_split': [0.1, 0.5],\n",
    "         'model__min_samples_leaf': [0.1, 0.5],\n",
    "         'model__max_depth':[3, 8],\n",
    "         'model__max_features':[\"log2\",\"sqrt\"],\n",
    "         'model__criterion': [\"friedman_mse\",  \"mae\"],\n",
    "         'model__subsample':[0.5, 1.0],\n",
    "         'model__n_estimators':[10]}]\n",
    "\n",
    "model_gradientboosting = model_train(ohe_X_train, y_train, gradientboosting_model, gradientboosting_grid, ohe_X_test, y_test, test_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training various models and tuning hyperparameters, we see that the `RandomForest` classifier achieved the highest accuracy with a pipeline test accuracy of `81.00%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data to be used for testing\n",
    "test_df = pd.read_csv('testing_values.csv')\n",
    "\n",
    "# remove labels from selected_features variable \n",
    "test_selected_features = selected_features.remove('status_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test data\n",
    "testing_data = data_preparation(test_df, selected_features, test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode the test data\n",
    "ohe_X_training, ohe_X_testing = one_hot_encode(features, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the final model on complete data set (training & validation)\n",
    "final_model = model_train(ohe_X_training, labels, rfc_model, rfc_grid, test_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the status group on the test data and export the list\n",
    "results = list(final_model.predict(ohe_X_testing)).to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score results on drivendata.org:\n",
    "<img src=\"drivendatascore.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
